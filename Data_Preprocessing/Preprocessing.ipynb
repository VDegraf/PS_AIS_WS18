{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritte:\n",
    "\n",
    "1. Einlesen der Reviews\n",
    "2. Preprocessing Schritte durchführen\n",
    "3. Ausgeben der vereinfachten Liste \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Einlesen der Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Importiere der benötigten Bibliotheken.\n",
    "Installation der benötigten Module.\n",
    "Konsoleneingabe:\n",
    "\n",
    "pip install contractions\n",
    "pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # necessary package for word_tokenize\n",
    "nltk.download('stopwords') # necessary package for remove_stopwords\n",
    "nltk.download('wordnet') # necessary package for lemmatize_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren der Reviews in ein Datafile (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'r')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "df = list(parse('../Data_raw/reviews_Musical_Instruments_5.json.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testausgabe des ersten Reviewtext aus dem Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0][\"reviewText\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Schritte durchführen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandelt englische Wörter wie: \"you've, it's, don't\" zu deren ausgeschriebener Grundform um.\n",
    "\n",
    "Eingabe: Belieber Text als String:\n",
    "\"It's a nice day.\"\n",
    "\n",
    "Ausgabe: Text als String:\n",
    "\"It is a nice day\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, it is a very nice day.\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, it's a very nice day.\"\n",
    "print(replace_contractions(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandelt alle Wörter und Satzzeichen eines Textes in eine Liste mit einzelenen Wörtern um.\n",
    "\n",
    "Eingabe: Beliebiger Text als String: \n",
    "\"It's a nice day.\"\n",
    "\n",
    "Ausgabe: Liste mit Wörtner und Satzzeichen als Listenelemente: \n",
    "['Hello', ',', 'it', \"'s\", 'a', 'very', 'nice', 'day', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'it', \"'s\", 'a', 'very', 'nice', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Hello, it's a very nice day.\"\n",
    "print(tokenize_words(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Non-ascii entfernen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entfernt alle nicht Ascii zeichen aus einer Liste mit tokenized Wörten.\n",
    "\n",
    "Eingabe: Liste mit tokenized Words: ['Hello', ',', 'it', \"'s\", 'a', 'very', 'nice', 'day', '.']\n",
    "\n",
    "Ausgabe: Liste nur mit Ascii Zeichen: ['Hello', ',', 'it', \"'s\", 'a', 'very', 'nice', 'day', '.']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '', 'it', \"'s\", '', 'very', 'nice', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "liste1 = ['Hello', '我', 'it', \"'s\", '我', 'very', 'nice', 'day', '.']\n",
    "print(remove_non_ascii(liste1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Convert to Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konvertiert alle Zeichen der Liste in Kleinbuchstaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einagbe: Liste mit Klein-und Großbuchstaben: ['Hello', 'it', 'is', 'a', 'nice', 'day' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe: Liste mit Kleinbuchstaben: ['hello', 'it', 'is', 'a', 'nice', 'day' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'it', 'is', 'a', 'nice', 'day']\n"
     ]
    }
   ],
   "source": [
    "liste2 = ['Hello', 'it', 'is', 'a', 'nice', 'day' ]\n",
    "print (to_lowercase(liste2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entfernt alle Satzzeichen aus der Liste "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eingabe: Liste mit Satzzeichen: ['Hello','?',it', 'is', 'a', 'nice', 'day','.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe: Liste ohne Satzzeichen: ['Hello','it', 'is', 'a', 'nice', 'day',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'it', 'is', 'a', 'nice', 'day']\n"
     ]
    }
   ],
   "source": [
    "liste3 = ['Hello','?','it', 'is', 'a', 'nice', 'day','.']\n",
    "print(remove_punctuation(liste3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Replace Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ersetzt alle Intenger der Liste durch eine textuelle Darstellung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einagbe: Liste mit Intenger: ['Hello','it', 'is', 'a', 'nice', 'day', '10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe: Liste mit ausgeschrieben Inetnger: ['Hello','it', 'is', 'a', 'nice', 'day', 'ten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'it', 'is', 'a', 'nice', 'day', 'ten']\n"
     ]
    }
   ],
   "source": [
    "liste4 = ['Hello','it', 'is', 'a', 'nice', 'day', '10']\n",
    "print(replace_numbers(liste4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entfertn alle Stoppwörter aus der Liste (Wörter müssen kleinkeschrieben sein. Zuerst \"Convert to Lowercase\" anwenden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eingabe: Liste mit Stoppwörter: ['I','like ', 'reading', 'so', 'I', 'red']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe: Liste ohne Stoppwörter:['like', 'reading', 'red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like ', 'reading', 'I', 'red']\n"
     ]
    }
   ],
   "source": [
    "liste5 = ['I','like ', 'reading', 'so', 'I', 'red', 'you']\n",
    "print(remove_stopwords(liste5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Stem Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduzieren von  abgeleiteten Wörtern auf ihren Stamm, ihre Basis oder ihre Wurzelform - im Allgemeinen eine schriftliche Wortform. Konvertiert alle Zeichen der Liste in Kleinbuchstaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eingabe: [\"reading\", \"booking\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe: [\"read\", \"book\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'read', 'so', 'don', 'i', 'book']\n"
     ]
    }
   ],
   "source": [
    "liste6 = ['I','am', 'reading', 'so', 'done', 'I', 'booking']\n",
    "print(stem_words(liste6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Lemmatize Verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gruppierung der verschiedenen gebeugten Formen eines Wortes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandelt in Präsens um und reduziert Wörtern auf ihren Stamm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eingabe: [\"am\", \"done\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe: [\"be\", \"do\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'be', 'read', 'do', 'I', 'book']\n"
     ]
    }
   ],
   "source": [
    "liste7 = ['I','am', 'reading', 'done', 'I', 'booking']\n",
    "print(lemmatize_verbs(liste7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion für die Anwendung aller Data Preprocessing Schritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_steps(words):\n",
    "    words = replace_contractions(words) # wandelt \"you've, it's, don't\" zu deren ausgeschriebener Grundform um\n",
    "    words = tokenize_words(words) # wandelt alle Wörter und Satzzeichen eines Textes in eine Liste um\n",
    "    words = remove_non_ascii(words) # entfernt alle nicht Ascii Zeichen aus einer Liste mit tokenized Wörten (leere Strings bleiben)\n",
    "    words = to_lowercase(words) # konvertiert alle Zeichen der Liste in Kleinbuchstaben\n",
    "    words = remove_punctuation(words) # entfernt alle Satzzeichen aus der Liste\n",
    "    words = replace_numbers(words) # ersetzt alle Intenger der Liste durch eine textuelle Darstellung\n",
    "    words = remove_stopwords(words) # entfertn alle Stoppwörter aus der Liste (Wörter müssen kleinkeschrieben sein. Zuerst \"Convert to Lowercase\" anwenden)\n",
    "    # words = stem_words(words) # reduziert Wörtern auf ihren Stamm und konvertiert alle Zeichen der Liste in Kleinbuchstaben\n",
    "    # words = lemmatize_verbs(words) # wandelt in Präsens um und reduziert Wörtern auf ihren Stamm\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordner für gefilterte Reviewkategorien (.csv) anlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordner Data_filtered existiert bereits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.exists(\"../Data_filtered\"):\n",
    "    print(\"Ordner Data_filtered existiert bereits\")\n",
    "if not os.path.exists(\"../Data_filtered\"):\n",
    "    os.makedirs(\"../Data_filtered\")\n",
    "    print(\"Ordner Data_filtered wurde erfolgreich angelegt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion normalisiert eine Reviewkategorie und erstellt eine .csv Datei und speichert diese in dem Ordner Data_filtered ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def read_preprocess_csv(path, name):\n",
    "    if os.path.isfile(\"../Data_filtered/%s.csv\" %name):\n",
    "        return print(\"Die gefilterte CSV existiert bereits\")\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        print(\"Die gefilterte CSV wird angelegt\")\n",
    "        data_raw = list(parse(path))\n",
    "        reviewText = []\n",
    "        reviewRating = []\n",
    "        for i in range(len(data_raw)): \n",
    "            text = preprocessing_steps(data_raw[i][\"reviewText\"])\n",
    "            score = data_raw[i][\"overall\"]\n",
    "            reviewText.append(text)\n",
    "            reviewRating.append(score)\n",
    "        data_filtered = pd.DataFrame(data={\"review\": reviewText, \"rating\": reviewRating})\n",
    "        data_filtered.to_csv(\"../Data_filtered/%s.csv\" %name, sep=';',index=False)\n",
    "        end_time = time.time()\n",
    "        return print(\"Datei erfolgreich angelegt \\n\" 'Dauer: {:5.3f}min'.format((end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die gefilterte CSV wird angelegt\n",
      "Datei erfolgreich angelegt \n",
      "Dauer: 22.623min\n"
     ]
    }
   ],
   "source": [
    "read_preprocess_csv(\"../Data_raw/reviews_Musical_Instruments_5.json.gz\", \"reviews_Musical_Instruments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die gefilterte CSV existiert bereits\n"
     ]
    }
   ],
   "source": [
    "read_preprocess_csv(\"../Data_raw/reviews_Automotive_5.json.gz\", \"reviews_Automotive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
